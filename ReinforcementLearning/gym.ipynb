{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gym.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paryoja/deeplearningstudy/blob/master/ReinforcementLearning/gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "48tQZy3h5cmz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "먼저 gym을 설치합니다\n",
        "\n",
        "출처 https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=pdb2JwZy4jGj&forceEdit=true&offline=true&sandboxMode=true"
      ]
    },
    {
      "metadata": {
        "id": "LXGpCBsd40OS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cEYwIUOA9aYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "팩맨 설치"
      ]
    },
    {
      "metadata": {
        "id": "1zXakI747Xvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o1vQ9NtJ5b-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Helper function들 "
      ]
    },
    {
      "metadata": {
        "id": "TMIptBX77uMR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l1ghviJ7HYPM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "가상 화면을 하나 만듭니다"
      ]
    },
    {
      "metadata": {
        "id": "ltQFsSpe9o-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4hpCQT0XHbrn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "show_video는 저장된 비디오를 보여주는 역할\n",
        "wrap_env는 플레이를 비디오로 저장하는 역할"
      ]
    },
    {
      "metadata": {
        "id": "2yu2TEp39uPN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def show_video():\n",
        "    html=[]\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    print(len(mp4list))\n",
        "    if len(mp4list) > 0:\n",
        "        for mp4 in mp4list:\n",
        "            video = io.open(mp4, 'r+b').read()\n",
        "            encoded = base64.b64encode(video)\n",
        "\n",
        "            html.append(\n",
        "                '''<video alt=\"test\" autoplay \n",
        "                        controls style=\"height: 400px;\">\n",
        "                        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                     </video>'''.format(encoded.decode('ascii'))\n",
        "            )\n",
        "\n",
        "        ipythondisplay.display(HTML(data='\\n'.join(html)))\n",
        "\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dPEuNxf7tZs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pacman!"
      ]
    },
    {
      "metadata": {
        "id": "dO0mGvZg71h7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZAr2379x5amn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#check out the pacman action space!\n",
        "print(env.action_space)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YSA5bcJUKmS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "policy를 정해주는 네트워크 구성"
      ]
    },
    {
      "metadata": {
        "id": "1KkA6qFb97aC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 일단 입력은 observation -> 출력은 action인 network 설정\n",
        "import random\n",
        "\n",
        "class policy_network:\n",
        "    def __init__(self):\n",
        "        self.ph = tf.placeholder(tf.float32, shape=[None, 210, 160, 3])\n",
        "        self.filter_1 = tf.Variable(tf.random_normal([3, 3, 3, 10]))\n",
        "        self.filter_2 = tf.Variable(tf.random_normal([3, 3, 10, 10]))\n",
        "        self.filter_3 = tf.Variable(tf.random_normal([3, 3, 10, 10]))\n",
        "        self.W = tf.Variable(tf.random_normal([328640, env.action_space.n],\n",
        "                                             stddev=(1.0 / (328640.0 * env.action_space.n))), \n",
        "                             dtype=tf.float32)\n",
        "        self.b = tf.Variable(env.action_space.n, dtype=tf.float32)\n",
        "        \n",
        "        self.ph_a = tf.placeholder(tf.int32, shape=[None, 1], name='action')\n",
        "        self.reward = tf.placeholder(tf.float32, shape=[None, 1], name='reward')\n",
        "        \n",
        "    def build(self):\n",
        "        # 입력은 -128 을 한다음 128로 나눠서 -1~1 사이에 위치하도록 한다\n",
        "        norm_x = (self.ph - 128) / 128\n",
        "        # [bs, 208, 158, 10]\n",
        "        \n",
        "        x = tf.nn.conv2d(norm_x, self.filter_1, [1, 1, 1, 1], \"VALID\")\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.nn.conv2d(x, self.filter_2, [1, 1, 1, 1], \"SAME\")\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.nn.conv2d(x, self.filter_3, [1, 1, 1, 1], \"SAME\")\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.reshape(x, [-1, 328640])\n",
        "        self.x = tf.matmul(x, self.W) + self.b\n",
        "        self.sm = tf.nn.softmax(self.x)\n",
        "        \n",
        "        logit = tf.one_hot(self.ph_a, env.action_space.n)\n",
        "        self.loss = -tf.reduce_sum(tf.log(self.sm * logit) * self.reward)\n",
        "        \n",
        "        opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "        \n",
        "        self.train_op = opt.minimize(self.loss)\n",
        "    \n",
        "    def get_action(self, sess, observation):\n",
        "        \n",
        "        # 확률을 예측하고\n",
        "        prob = sess.run(self.sm, feed_dict={self.ph: observation})\n",
        "        # print(prob)\n",
        "        # 확률을 따라 action을 선택한다\n",
        "        return self.choice(prob[0]), prob\n",
        "        # [0.01, 0.09, 0.15, .... ]\n",
        "        # 0 ~ 0.01 -> 1 action\n",
        "        # 0.0100000001 -> ~ 0.10 -> 2 action\n",
        "        # 0.1000000001 ~ 0.25 -> 3 action\n",
        "    \n",
        "    def update_param(self, sess, observation, reward, action):\n",
        "        sess.run(self.train_op, feed_dict={\n",
        "            self.ph: observation,\n",
        "            self.ph_a: action,\n",
        "            self.reward: reward,\n",
        "        })\n",
        "    \n",
        "    def choice(self, prob):\n",
        "        r = random.random()\n",
        "        \n",
        "        for i, p in enumerate(prob):\n",
        "            r -= p\n",
        "            \n",
        "            if r < 0:\n",
        "                return i\n",
        "        return i\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8Ty5OgXKo3B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "value function의 네트워크 구성"
      ]
    },
    {
      "metadata": {
        "id": "sTN1TmFyKrVh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class value_network:\n",
        "    def __init__(self):\n",
        "        self.ph = tf.placeholder(tf.float32, shape=[None, 210, 160, 3])\n",
        "        self.filter_1 = tf.Variable(tf.random_normal([3, 3, 3, 10]))\n",
        "        self.filter_2 = tf.Variable(tf.random_normal([3, 3, 10, 10]))\n",
        "        self.filter_3 = tf.Variable(tf.random_normal([3, 3, 10, 10]))\n",
        "        self.W = tf.Variable(tf.random_normal([328640, 1],\n",
        "                                             stddev=(1.0 / 328640.0)), \n",
        "                             dtype=tf.float32)\n",
        "        self.b = tf.Variable(1, dtype=tf.float32)\n",
        "        \n",
        "        self.ph_a = tf.placeholder(tf.int32, shape=[None, 1], name='action')\n",
        "        self.reward = tf.placeholder(tf.float32, shape=[None, 1], name='reward')\n",
        "        \n",
        "    def build(self):\n",
        "        # 입력은 -128 을 한다음 128로 나눠서 -1~1 사이에 위치하도록 한다\n",
        "        norm_x = (self.ph - 128) / 128\n",
        "        # [bs, 208, 158, 10]\n",
        "        \n",
        "        x = tf.nn.conv2d(norm_x, self.filter_1, [1, 1, 1, 1], \"VALID\")\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.nn.conv2d(x, self.filter_2, [1, 1, 1, 1], \"SAME\")\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.nn.conv2d(x, self.filter_3, [1, 1, 1, 1], \"SAME\")\n",
        "        x = tf.nn.relu(x)\n",
        "        x = tf.reshape(x, [-1, 328640])\n",
        "        self.x = tf.matmul(x, self.W) + self.b\n",
        "        self.sm = tf.nn.softmax(self.x)\n",
        "        \n",
        "        logit = tf.one_hot(self.ph_a, env.action_space.n)\n",
        "        self.loss = -tf.reduce_sum(tf.log(self.sm * logit) * self.reward)\n",
        "        \n",
        "        opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "        \n",
        "        self.train_op = opt.minimize(self.loss)\n",
        "    \n",
        "    def get_action(self, sess, observation):\n",
        "        \n",
        "        # 확률을 예측하고\n",
        "        prob = sess.run(self.sm, feed_dict={self.ph: observation})\n",
        "        # print(prob)\n",
        "        # 확률을 따라 action을 선택한다\n",
        "        return self.choice(prob[0]), prob\n",
        "        # [0.01, 0.09, 0.15, .... ]\n",
        "        # 0 ~ 0.01 -> 1 action\n",
        "        # 0.0100000001 -> ~ 0.10 -> 2 action\n",
        "        # 0.1000000001 ~ 0.25 -> 3 action\n",
        "    \n",
        "    def update_param(self, sess, observation, reward, action):\n",
        "        sess.run(self.train_op, feed_dict={\n",
        "            self.ph: observation,\n",
        "            self.ph_a: action,\n",
        "            self.reward: reward,\n",
        "        })\n",
        "    \n",
        "    def choice(self, prob):\n",
        "        r = random.random()\n",
        "        \n",
        "        for i, p in enumerate(prob):\n",
        "            r -= p\n",
        "            \n",
        "            if r < 0:\n",
        "                return i\n",
        "        return i\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVGbPof3x-rM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = policy_network()\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "d.build()\n",
        "replay = []\n",
        "avg_replay = []\n",
        "final_reward = []\n",
        "\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mPo5q14iFWCI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "학습해보자"
      ]
    },
    {
      "metadata": {
        "id": "_2-q671p7Muj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make(\"MsPacman-v0\"))\n",
        "\n",
        "episode = 5\n",
        "\n",
        "\n",
        "gamma = 0.99\n",
        "\n",
        "total_length = 0\n",
        "avg_reward = 0\n",
        "\n",
        "for ep in range(episode):\n",
        "    play = []\n",
        "    observation = env.reset()\n",
        "    \n",
        "    prev_live = 0\n",
        "    action = 0\n",
        "    \n",
        "    stay_action = 0\n",
        "    \n",
        "    while True:\n",
        "\n",
        "        #env.render()\n",
        "\n",
        "        # your agent goes here\n",
        "        # action = env.action_space.sample() \n",
        "        if stay_action == 0:\n",
        "            if random.random() > 0.5:\n",
        "                action, _ = d.get_action(sess, [observation])\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "            stay_action = 3\n",
        "        else:\n",
        "            stay_action -= 1\n",
        "        \n",
        "        # print(action)\n",
        "        prev_ob = observation\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        \n",
        "        if prev_live != info['ale.lives'] and info['ale.lives'] != 0:\n",
        "            for i in range(60):\n",
        "                # 60 frame (2초) 은 의미 없어서 스킵\n",
        "                env.step(env.action_space.sample())\n",
        "            prev_live = info['ale.lives']\n",
        "            \n",
        "            \n",
        "     \n",
        "        \n",
        "        # print(\"action {}, reward {}\".format(action, reward))\n",
        "        play.append([prev_ob, action, observation, reward, info])\n",
        "\n",
        "        if done: \n",
        "          break;\n",
        "    replay.append(play)\n",
        "    print(info)    \n",
        "    \n",
        "    \n",
        "    print(np.shape(play[0][0]))\n",
        "    print(len(play))\n",
        "    \n",
        "    total_length += len(play)\n",
        "    \n",
        "    moving_average = -100 # 죽기 직전 행동은 안좋은 것으로 세팅\n",
        "    total_reward = 0\n",
        "    prev_lives = 0\n",
        "    # 거꾸로 보면서 누적 reward 계산\n",
        "    for p in reversed(play):\n",
        "        # base function 을 -10으로 세팅\n",
        "        if p[4]['ale.lives'] != prev_lives:\n",
        "            moving_average = -100\n",
        "            prev_lives = p[4]['ale.lives']\n",
        "        \n",
        "        moving_average = gamma * moving_average + (p[3] - 10)\n",
        "        total_reward += p[3]\n",
        "        p.append(moving_average)\n",
        "    \n",
        "    final_reward.append(total_reward)\n",
        "    print(total_reward)\n",
        "    if len(replay) > 100:\n",
        "        del replay[0]\n",
        "        del final_reward[0]\n",
        "    avg_reward += total_reward / episode\n",
        "avg_replay.append((total_length / episode, avg_reward))\n",
        "#     print(play[0][1])\n",
        "#     print(play[0][2])\n",
        "env.close()\n",
        "show_video()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvZCZfs3syqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "MG4ebCxj8NuO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(replay)):\n",
        "    minibatch_ob = []\n",
        "    minibatch_reward = []\n",
        "    minibatch_action = []\n",
        "    total_reward = []\n",
        "    \n",
        "    for k in range(100):\n",
        "        r = random.randint(0, len(replay) - 1)\n",
        "        s = random.randint(0, len(replay[r]) - 1)\n",
        "    \n",
        "        x = replay[r][s]\n",
        "        minibatch_ob.append(x[2])\n",
        "        minibatch_reward.append(x[-1])\n",
        "        minibatch_action.append(x[1])\n",
        "        \n",
        "        total_reward.append(final_reward[r])\n",
        "    # print(x[1], x[-1])\n",
        "    d.update_param(sess, minibatch_ob, np.reshape(minibatch_reward, [-1,1]), np.reshape(minibatch_action, [-1, 1]))    \n",
        "    \n",
        "    if i % 10 == 9:\n",
        "        print(i)\n",
        "        end = time.time()\n",
        "        print(end - start)\n",
        "        start = end"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-4Jss7R8D3iP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(avg_replay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qj1RcWOFCTvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}